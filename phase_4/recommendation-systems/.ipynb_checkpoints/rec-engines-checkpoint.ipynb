{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engines: Implementing Surprise\n",
    "- Surprise is a scikit library that stands for **Simple Python RecommendatIon System Engine**\n",
    "- Has built-in similarity metrics, baseline methods, content-based systems, matrix factorization systems\n",
    "\n",
    "In this notebook, we'll first walk through setting up a super basic recommendation system, using the popular MovieLens 100K Dataset. Then, we'll look into more detail how Surprise works.\n",
    "\n",
    "## Fitting and Predicting with Surprise\n",
    "\n",
    "### 1. Install surprise if you haven't, and import the usual libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting surprise\n",
      "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
      "Collecting scikit-surprise\n",
      "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 6.9 MB/s eta 0:00:01    |███████████████████▊            | 7.2 MB 2.8 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /Users/josh/anaconda3/lib/python3.8/site-packages (from scikit-surprise->surprise) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.2 in /Users/josh/anaconda3/lib/python3.8/site-packages (from scikit-surprise->surprise) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/josh/anaconda3/lib/python3.8/site-packages (from scikit-surprise->surprise) (1.6.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/josh/anaconda3/lib/python3.8/site-packages (from scikit-surprise->surprise) (1.15.0)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl size=785626 sha256=85bc35609651068c4efac666c0e38c5aea7adc6a823100b8d3f85f9819e3616f\n",
      "  Stored in directory: /Users/josh/Library/Caches/pip/wheels/20/91/57/2965d4cff1b8ac7ed1b6fa25741882af3974b54a31759e10b6\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scikit-surprise, surprise\n",
      "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load in the dataset\n",
    "\n",
    "Surprise has the dataset built in. You might need to download the dataset so follow the instructions in the code output! Unfortunately, the Surprise data format doesn't let us inspect the data, but here is the documentation: https://grouplens.org/datasets/movielens/100k/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ml-100k could not be found. Do you want to download it? [Y/n] y\n",
      "Trying to download dataset from http://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
      "Done! Dataset ml-100k has been saved to /Users/josh/.surprise_data/ml-100k\n"
     ]
    }
   ],
   "source": [
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# train-test split\n",
    "train, test = train_test_split(data, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.trainset.Trainset at 0x7f889ea45250>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the default Singular Value Decomposition Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = SVD()\n",
    "svd.fit(train)\n",
    "predictions = svd.test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = str(196)  # raw user id (as in the ratings file). They are **strings**!\n",
    "iid = str(302)\n",
    "\n",
    "# get a prediction for specific users and items.\n",
    "pred = svd.predict(uid, iid, r_ui=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Surprise\n",
    "\n",
    "### 1. How does Surprise take in your data?\n",
    "https://surprise.readthedocs.io/en/stable/getting_started.html#use-a-custom-dataset\n",
    "\n",
    "The dataset we'll use is a subset of the Yelp Open Dataset that's already been joined and cleaned.\n",
    "https://www.yelp.com/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp = pd.read_csv('yelp_reviews.csv').drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspecting the dataset:\n",
    "\n",
    "Here's where you'd do a **comprehensive** EDA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of Users: ', len(yelp['user_id'].unique()))\n",
    "print('Number of Businesses: ', len(yelp['business_id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What's the distribution of ratings? i.e. How many 1-star, 2-star, 3-star reviews?\n",
    "2. How many reviews does a restaurant have?\n",
    "3. How many reviews does a user make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['business_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reading in the dataset and prepping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a 'Reader' to read in the data so Surprise can use it\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(yelp[['user_id', 'business_id', 'stars']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fitting and evaluating models\n",
    "Here, let's assume that we've tuned all these hyperparameters using GridSearch, and we've arrived at our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = SVD(n_epochs=20, n_factors=1, biased=True, \n",
    "              lr_all=0.005, reg_all=0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = final.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Making Predictions (again)\n",
    "Unfortunately, this dataset has a convoluted string as the user/business IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['user_id'][55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['business_id'][123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final.predict(yelp['user_id'][55], yelp['business_id'][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What else?\n",
    "\n",
    "Surprise has sample code where you can get the top **n** recommended items for a user. https://surprise.readthedocs.io/en/stable/FAQ.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The structure of our lesson on recommendation engines is based on Chapter 9 of **Mining of Massive Datasets**: http://infolab.stanford.edu/~ullman/mmds/book.pdf\n",
    "- Libraries for coding recommendation engines: \n",
    "    - Surprise: https://surprise.readthedocs.io/en/stable/index.html\n",
    "    - LightFM: https://lyst.github.io/lightfm/docs/index.html\n",
    "    \n",
    "    \n",
    "- Some blogs I might've written:\n",
    "    - Overview: https://towardsdatascience.com/a-primer-to-recommendation-engines-49bd12ed849f?source=friends_link&sk=279dfeec5187614b37431dab167fd4e3\n",
    "    - Collaborative filtering: https://towardsdatascience.com/a-primer-to-recommendation-engines-49bd12ed849f?source=friends_link&sk=279dfeec5187614b37431dab167fd4e3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
