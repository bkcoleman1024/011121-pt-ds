{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    "# 24: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 1
   },
   "source": [
    "## What is Regularization?\n",
    "In an attempt to fit a good model to data, we often tend to overfit. In this section, you will learn about Regularization, a technique used to avoid overfitting. Regularization discourages overly complex models by penalizing the loss function.\n",
    "\n",
    "## Recap: Why Regularize?\n",
    "\n",
    "### The Bias-Variance Tradeoff\n",
    "\n",
    "When we did Linear Regression, we briefly talked about the BV Tradeoff.\n",
    "\n",
    "![](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n",
    "\n",
    "![](https://miro.medium.com/max/544/1*Y-yJiR0FzMgchPA-Fm5c1Q.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 2
   },
   "source": [
    "1. **High bias** \n",
    "    * Systematic error in predictions (i.e. the average)\n",
    "    * Bias is about the strength of assumptions the model makes\n",
    "    * Underfit models tend to have high bias\n",
    "\n",
    "\n",
    "2. **High variance**\n",
    "    * The model is highly sensitive to changes in the data\n",
    "    * Overfit models tend to have low bias and high variance\n",
    "    \n",
    "    \n",
    "![](https://gblobscdn.gitbook.com/assets%2F-LvBP1svpACTB1R1x_U4%2F-LvNWUoWieQqaGmU_gl9%2F-LvNoby-llz4QzAK15nL%2Fimage.png?alt=media&token=41720ce9-bb66-4419-9bd8-640abf1fc415)\n",
    "\n",
    "* Underfit Models fail to capture all of the information in the data\n",
    "    * Ex. the mean leaves a lot of information on the table\n",
    "* Overfit models fit to the noise in the data and fail to generalize\n",
    "* How would we know if our model is over or underfit?\n",
    "    * Train test split\n",
    "    * Look at the testing error\n",
    "    * As model complexity increases so does the possibility for overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 3
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 4
   },
   "source": [
    "*The below explanation is thanks to Anuja Nagpal's awesome article: [L1 and L2 Regularization Methods](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) in Towards Data Science*\n",
    "\n",
    "Regularization methods in regression models come in two types:\n",
    "\n",
    "\n",
    "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n",
    "\n",
    "\n",
    "The key difference between these two is the penalty term.\n",
    "\n",
    "\n",
    "### **Ridge regression** or L2\n",
    "\n",
    "adds “squared magnitude” of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.\n",
    "\n",
    "### Cost function\n",
    "![Ridge_math](ridge_math.png)\n",
    "\n",
    "\n",
    "Here, if lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue.\n",
    "\n",
    "### **Lasso Regression** (Least Absolute Shrinkage and Selection Operator) or L1\n",
    "\n",
    "adds “absolute value of magnitude” of coefficient as penalty term to the loss function.\n",
    "\n",
    "### Cost function\n",
    "![lasso_math](lasso_math.png)\n",
    "\n",
    "Again, if lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit.\n",
    "The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 5
   },
   "source": [
    "# Why Regularize?\n",
    "\n",
    "Remember overfitting?  One way models overfit is by becoming too complex, they may assign very different weights, or coefficients, to different features.  Both L1 and L2 regularization push weights toward zero, simplifying the model by removing or minimizing the effect of coefficients that are of less importance, thus simplifying a model.\n",
    "\n",
    "L2 regularization also penalizes very large weights more than smaller ones.  This prevents a model from putting too much emphasis on one or a few features while ignoring others that may also be relevant.\n",
    "\n",
    "L1 and L2 regularization can each be tried in separate models, or they can be combined in a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 6
   },
   "source": [
    "### Standardization before Regularization\n",
    "\n",
    "An important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so **if you have features that are on different scales, some will get unfairly penalized**. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\n",
    "\n",
    "**Scaler documentation:**\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 7
   },
   "source": [
    "## Let's Code! \n",
    "\n",
    "Start with a regular Linear Regression and note some important things about scikit-learn's syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 8
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 9
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../resources/Housing_Prices/train.csv')\n",
    "\n",
    "# Create X and y\n",
    "y = df['SalePrice']\n",
    "X = df.drop(columns=['SalePrice'], axis=1)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
    "\n",
    "# Remove \"object\"-type features from X\n",
    "cont_features = [col for col in X.columns if X[col].dtype in [np.float64, np.int64]]\n",
    "\n",
    "# Remove \"object\"-type features from X_train and X_test\n",
    "X_train_cont = X_train.loc[:, cont_features]\n",
    "X_test_cont = X_test.loc[:, cont_features]\n",
    "\n",
    "X_train_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 10
   },
   "outputs": [],
   "source": [
    "# Impute missing values with median using SimpleImputer\n",
    "impute = SimpleImputer(strategy='median') \n",
    "X_train_imputed = impute.fit_transform(X_train_cont) # what is fit_transform?\n",
    "X_test_imputed = impute.transform(X_test_cont) # vs just .transform?\n",
    "\n",
    "\n",
    "# Create X_cat which contains only the categorical variables\n",
    "features_cat = [col for col in X.columns if X[col].dtype in [np.object]]\n",
    "X_train_cat = X_train.loc[:, features_cat]\n",
    "X_test_cat = X_test.loc[:, features_cat]\n",
    "\n",
    "# Fill missing values with the string 'missing'\n",
    "X_train_cat.fillna(value='missing', inplace=True)\n",
    "X_test_cat.fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Scale the train and test data\n",
    "ss = StandardScaler()\n",
    "X_train_imputed_scaled = ss.fit_transform(X_train_imputed)\n",
    "X_test_imputed_scaled = ss.transform(X_test_imputed)\n",
    "\n",
    "# Fit the model \n",
    "linreg_norm = LinearRegression()\n",
    "linreg_norm.fit(X_train_imputed_scaled, y_train)\n",
    "\n",
    "\n",
    "# OneHotEncode categorical variables\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Transform training and test sets\n",
    "X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)\n",
    "\n",
    "# Convert these columns into a DataFrame \n",
    "columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "\n",
    "X_train_all = pd.concat([pd.DataFrame(X_train_imputed_scaled), cat_train_df], axis=1)\n",
    "X_test_all = pd.concat([pd.DataFrame(X_test_imputed_scaled), cat_test_df], axis=1)\n",
    "\n",
    "\n",
    "# Fitting the model\n",
    "linreg_all = LinearRegression()\n",
    "linreg_all.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', linreg_all.score(X_train_all, y_train))\n",
    "print('Test r^2:', linreg_all.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, linreg_all.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, linreg_all.predict(X_test_all)))\n",
    "print('Training RMSE:', mean_squared_error(y_train, linreg_all.predict(X_train_all))**0.5)\n",
    "print('Test RMSE:', mean_squared_error(y_test, linreg_all.predict(X_test_all))**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 11
   },
   "source": [
    "## Fitting Ridge and Lasso\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 12
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 13
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso() # Lasso is also known as the L1 norm \n",
    "lasso.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', lasso.score(X_train_all, y_train))\n",
    "print('Test r^2:', lasso.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, lasso.predict(X_test_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 14
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=10) # adjusting HYPERPARAMETERS -- check documentation!\n",
    "lasso.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', lasso.score(X_train_all, y_train))\n",
    "print('Test r^2:', lasso.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, lasso.predict(X_test_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 15
   },
   "outputs": [],
   "source": [
    "# how many coefficients were brought to zero?\n",
    "\n",
    "print(\"Total number of coefficients: \", len(lasso.coef_))\n",
    "print(\"Coefficients close to zero: \", sum(abs(lasso.coef_) < 10**(-10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 16
   },
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 17
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=10) # Ridge is also known as the L2 norm\n",
    "ridge.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', ridge.score(X_train_all, y_train))\n",
    "print('Test r^2:', ridge.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, ridge.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, ridge.predict(X_test_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 18
   },
   "source": [
    "## Ridge & Lasso: Other benefits\n",
    "\n",
    "### Ridge:\n",
    "* We can \"shrink down\" prediction variables effects instead of deleting/zeroing them\n",
    "* When you have features with high multicollinearity, the coefficients are automatically spread across them (you won't have redundancy)\n",
    "* Since includes all features it can be computationally expensive (for many variables)\n",
    "\n",
    "### Lasso:\n",
    "* When you have a lot of variables it performs feature selection for you!\n",
    "* Multicollinearity is also dealt with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 19
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
