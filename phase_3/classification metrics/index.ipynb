{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T21:03:49.549004Z", "start_time": "2021-04-08T21:03:49.545632Z"}, "index": 0}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n", "from sklearn.datasets import load_iris, make_classification\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.linear_model import LogisticRegression\n", "from seaborn import heatmap"]}, {"cell_type": "markdown", "metadata": {"index": 1}, "source": ["# On Classifier Evaluation"]}, {"cell_type": "markdown", "metadata": {"index": 2}, "source": ["## Confusion Matrix\n", "\n", "For classification problems, the target is a categorical variable. This means that we can simply count the number of times that our model predicts the correct category and the number of times that it predicts something else.\n", "\n", "We can visualize this by means of a **confusion matrix**, which displays counts of correct and incorrect predictions. We'll explore this below. There are [many ways](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks) of evaluating a classification model, but most derive from the confusion matrix."]}, {"cell_type": "markdown", "metadata": {"index": 3}, "source": ["## Lottery Number Prediction\n", "\n", "Suppose I want to train a model to predict whether a string of six numbers (a \"ticket\") would win the lottery or not. What sort of data might I use?\n", "\n", "### Scenario 1\n", "\n", "I gather all the winning tickets from the last 10000 days or so. So I have one column for the tickets themselves, and a Boolean column indicating whether the ticket was a winner or not.\n", "\n", "Now if all the tickets on which my model trains are *winning* tickets, then it would predict every ticket to win! Suppose I test the model on a set of 1000 tickets, and suppose that there is exactly one winning ticket among those 1000. My model will always predict the ticket to win. Let's think about what the confusion matrix will look like."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 4}, "outputs": [], "source": ["# Setting up the true values:\n", "# create an array shape (1000,1) full of 'False' values\n", "# randomly select one value to be true (winning ticket)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 6}, "outputs": [], "source": ["# Setting up the predictions\n", "# Create another array of the same size as y_true with all ones."]}, {"cell_type": "markdown", "metadata": {"index": 8}, "source": ["# Confusion Matrices\n", "\n", "Confusion matrices are the basis of many of the metrics for classification.  There are a few exceptions.\n", "\n", "![Confusion Matrix Image](../resources/confusion_matrix.png)\n", "\n", "Image courtesy of [Open Genius IQ](https://iq.opengenus.org/performance-metrics-in-classification-regression/)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:56:34.322153Z", "start_time": "2021-04-08T20:56:34.317928Z"}, "index": 9}, "outputs": [], "source": ["# Defining the confusion matrix\n", "cm_1 = confusion_matrix(y_true, y_pred)"]}, {"cell_type": "markdown", "metadata": {"index": 10}, "source": ["The confusion matrix should tell us that we have 999 false positives (999 losing tickets predicted to win) and 1 true positive (1 winning ticket predicted to win):"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:24.234082Z", "start_time": "2021-04-08T20:46:24.230672Z"}, "index": 11}, "outputs": [], "source": ["cm_1"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 12}, "outputs": [], "source": ["heatmap(cm_1, annot=True, cmap='Reds', \n", "        xticklabels=['predict loss','predict win'],\n", "        yticklabels=['true loss','true win'])"]}, {"cell_type": "markdown", "metadata": {"index": 13}, "source": ["Notice the way that sklearn displays its confusion matrix: The rows are \\['actually false', 'actually true'\\]; the columns are \\['predicted false', 'predicted true'\\].\n", "\n", "So it displays:\n", "\n", "$\\begin{bmatrix}\n", "TN & FP \\\\\n", "FN & TP\n", "\\end{bmatrix}$"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:25.059195Z", "start_time": "2021-04-08T20:46:25.056363Z"}, "index": 14}, "outputs": [], "source": ["tn = cm_1[0, 0]\n", "fp = cm_1[0, 1]\n", "fn = cm_1[1, 0]\n", "tp = cm_1[1, 1]"]}, {"cell_type": "markdown", "metadata": {"index": 15}, "source": ["Let's see if we can calculate some of our metrics for this matrix."]}, {"cell_type": "markdown", "metadata": {"index": 16}, "source": ["## **Accuracy** = $\\frac{TP + TN}{TP + TN + FP + FN}$\n", "\n", "In words: How often did my model get the right answer?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:58:49.953817Z", "start_time": "2021-04-08T20:58:49.951601Z"}, "index": 17}, "outputs": [], "source": ["def accuracy_score():\n", "    #your function here\n", "    \n", "lottery_accuracy = accuracy_score()"]}, {"cell_type": "markdown", "metadata": {"index": 18}, "source": ["## **Recall** = $\\frac{TP}{TP + FN}$\n", "\n", "In words: How many of the true positives did my model label as predicted positives?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:59:02.543563Z", "start_time": "2021-04-08T20:59:02.541297Z"}, "index": 19}, "outputs": [], "source": ["def recall_score():\n", "    #your function here\n", "    \n", "lottery_recall = recall_score()"]}, {"cell_type": "markdown", "metadata": {"index": 20}, "source": ["## **Precision** = $\\frac{TP}{TP + FP}$\n", "\n", "In words: How many of the predicted positives where true positives?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:59:14.753452Z", "start_time": "2021-04-08T20:59:14.751075Z"}, "index": 21}, "outputs": [], "source": ["def precision_score():\n", "    #your function here\n", "    \n", "lottery_precision = precision_score()"]}, {"cell_type": "markdown", "metadata": {"index": 22}, "source": ["## **F-1 Score** = $\\frac{2PrRc}{Pr + Rc}$ = $\\frac{2TP}{2TP + FP + FN}$\n", "\n", "This one is harder to put into words, but think of it as a balance of recall and precision."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:59:33.929569Z", "start_time": "2021-04-08T20:59:33.927436Z"}, "index": 23}, "outputs": [], "source": ["def f1_score():\n", "    #your function here\n", "\n", "lottery_f1 = f1_score()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 24}, "outputs": [], "source": ["print('accuracy = ', lottery_accuracy)\n", "print('recall = ', lottery_recall)\n", "print('precision = ', lottery_precision)\n", "print('f1 = ', lottery_f1)"]}, {"cell_type": "markdown", "metadata": {"index": 25}, "source": ["### Scenario 2\n", "\n", "Well, my recall was good, but everything else I measured was terrible! Can I do better?\n", "\n", "This time I'll train my model in a much different way. I'll give it the tickets of 10000 people who played the lottery yesterday. Suppose that there are one winning ticket and 9999 losers. Now I test the model on the same 1000 tickets from before in Scenario 1.\n", "\n", "This time my model will almost always predict a ticket to lose. Suppose that, in the 1000 predictions, it makes only one prediction of a winner, and suppose that this prediction is wrong."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T21:06:36.957625Z", "start_time": "2021-04-08T21:06:36.954764Z"}, "index": 26}, "outputs": [], "source": ["# Set up predictions\n", "y_pred_2 = np.zeros((1000,1))\n", "pred_winner = np.random.randint(0,1001)\n", "y_pred_2[pred_winner] = 1"]}, {"cell_type": "markdown", "metadata": {"index": 27}, "source": ["Instead of coding out the scores by hand, we can use sklearn to speed things up."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 28}, "outputs": [], "source": ["# Import accuracy_score, precision_score, recall_score, and f1_score from sklearn.metrics\n", "#"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T21:08:21.098797Z", "start_time": "2021-04-08T21:08:21.088709Z"}, "index": 30}, "outputs": [], "source": ["# Print out the scores for each metric\n", "accuracy = accuracy_score(y_true, y_pred_2)\n", "precision = precision_score(y_true, y_pred_2)\n", "recall = recall_score(y_true, y_pred_2)\n", "f1 = f1_score(y_true, y_pred_2)\n", "\n", "conf = confusion_matrix(y_true, y_pred_2)\n", "heatmap(conf, annot=True)\n", "\n", "print('Accuracy', accuracy)\n", "print('Precision', precision)\n", "print('Recall', recall)\n", "print('F1', f1)"]}, {"cell_type": "markdown", "metadata": {"index": 31}, "source": ["## Resampling\n", "\n", "The last classifier had a really high accuracy, but everything else was terrible.\n", "\n", "In both cases, the problem we ran into was **class imbalance**. A popular solution for such a problem is resampling our data. To do this, let's take a look at [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:31.561140Z", "start_time": "2021-04-08T20:46:31.141686Z"}, "index": 32}, "outputs": [], "source": ["from imblearn.over_sampling import SMOTE"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:31.659931Z", "start_time": "2021-04-08T20:46:31.635872Z"}, "index": 33}, "outputs": [], "source": ["data = make_classification(n_samples = 10000, weights=[0.1, 0.9])\n", "X = data[0]\n", "y = data[1]\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2021)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:32.057482Z", "start_time": "2021-04-08T20:46:32.041409Z"}, "index": 34}, "outputs": [], "source": ["# Check class balance for the training data\n", "pd.Series(y_train).value_counts(normalize=True)"]}, {"cell_type": "markdown", "metadata": {"index": 35}, "source": ["This is a pretty imbalanced dataset. Only 10% of the data has a class of `0`.\n", "\n", "Let's use resampling to address this:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:32.841299Z", "start_time": "2021-04-08T20:46:32.812557Z"}, "index": 36}, "outputs": [], "source": ["resample = SMOTE(random_state=2021)\n", "X_train_resampled, y_train_resampled = resample.fit_resample(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {"index": 37}, "source": ["## Never Resample Your Test Set!\n", "\n", "### **Also, when using cross validation, ALWAYS use a pipeline when resampling data, same as any transformer**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:33.191466Z", "start_time": "2021-04-08T20:46:33.185722Z"}, "index": 38}, "outputs": [], "source": ["pd.Series(y_train_resampled).value_counts(normalize=True)"]}, {"cell_type": "markdown", "metadata": {"index": 39}, "source": ["## Multiple Classes\n", "\n", "We can understand these metrics of recall, precision, and the rest even if there are more than two classes in our classification problem."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:45.914762Z", "start_time": "2021-04-08T20:46:45.909918Z"}, "index": 40}, "outputs": [], "source": ["flowers = load_iris()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:47.153420Z", "start_time": "2021-04-08T20:46:47.150104Z"}, "index": 41}, "outputs": [], "source": ["print(flowers.DESCR)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:55.002329Z", "start_time": "2021-04-08T20:46:54.998470Z"}, "index": 42}, "outputs": [], "source": ["dims_train, dims_test, spec_train, spec_test = train_test_split(flowers.data,\n", "                                                                flowers.target,\n", "                                                                test_size=0.5,\n", "                                                               random_state=2021)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:46:59.538976Z", "start_time": "2021-04-08T20:46:59.535048Z"}, "index": 43}, "outputs": [], "source": ["spec_train[:5]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:47:07.328374Z", "start_time": "2021-04-08T20:47:07.323982Z"}, "index": 44}, "outputs": [], "source": ["ss_f = StandardScaler()\n", "\n", "ss_f.fit(dims_train)\n", "\n", "dims_train_sc = ss_f.transform(dims_train)\n", "dims_test_sc = ss_f.transform(dims_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:47:12.543744Z", "start_time": "2021-04-08T20:47:12.531010Z"}, "index": 45}, "outputs": [], "source": ["logreg_f = LogisticRegression(multi_class='multinomial',\n", "                             C=0.01, random_state=42)\n", "\n", "logreg_f.fit(dims_train_sc, spec_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:47:18.478116Z", "start_time": "2021-04-08T20:47:18.318759Z"}, "index": 46}, "outputs": [], "source": ["plot_confusion_matrix(estimator=logreg_f,\n", "                      X=dims_test_sc,\n", "                      y_true=spec_test,\n", "                     display_labels=[\n", "                         'setosa',\n", "                         'versicolor',\n", "                         'virginica'\n", "                            ]);"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:48:06.656182Z", "start_time": "2021-04-08T20:48:06.651992Z"}, "index": 47}, "outputs": [], "source": ["accuracy_score(spec_test,\n", "              logreg_f.predict(dims_test_sc))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:48:18.348681Z", "start_time": "2021-04-08T20:48:18.342834Z"}, "index": 48}, "outputs": [], "source": ["precision_score(spec_test,\n", "                logreg_f.predict(dims_test_sc),\n", "               average=None)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T20:48:34.038017Z", "start_time": "2021-04-08T20:48:34.032186Z"}, "index": 49}, "outputs": [], "source": ["recall_score(spec_test,\n", "            logreg_f.predict(dims_test_sc),\n", "            average=None\n", "            )"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 50}, "outputs": [], "source": ["f1_score(spec_test,\n", "            logreg_f.predict(dims_test_sc),\n", "            average=None)"]}, {"cell_type": "markdown", "metadata": {"index": 51}, "source": ["### Why are we getting three numbers for these metrics instead of one???"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["*YOUR ANSWER HERE*"]}, {"cell_type": "markdown", "metadata": {"index": 53}, "source": ["If our data is not binary, we have to change the `average=` argument to something other than 'binary', which is the default. "]}, {"cell_type": "markdown", "metadata": {"index": 54}, "source": ["For multi-class precision, the relevant denominator is a **column**; for multi-class recall, the relevant denominator is a **row**."]}, {"cell_type": "markdown", "metadata": {"index": 55}, "source": ["## Which Metric Should I Care About?\n", "\n", "Well, it depends.\n", "\n", "### General Lessons\n", "\n", "First, let's make some general observations about the metrics we've so far defined.\n", "\n", "Accuracy:\n", "- Pro: Takes into account both false positives and false negatives.\n", "- Con: Can be misleadingly high when there is a significant class imbalance. (A lottery-ticket predictor that *always* predicts a loser will be highly accurate.)\n", "\n", "Recall:\n", "- Pro: Highly sensitive to false negatives.\n", "- Con: No sensitivity to false positives.\n", "\n", "Precision:\n", "- Pro: Highly sensitive to false positives.\n", "- Con: No sensitivity to false negatives.\n", "\n", "F-1 Score:\n", "- Harmonic mean of recall and precision.\n", "    - Pro: Balanced both recall and precision\n", "    - Con: Harder to interpret/explain"]}, {"cell_type": "markdown", "metadata": {"index": 56}, "source": ["### Using classification metrics with sklearn's cross validation"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T21:18:32.245194Z", "start_time": "2021-04-08T21:18:32.238835Z"}, "index": 57}, "outputs": [], "source": ["# Import some classification data\n", "data = make_classification()\n", "X = data[0]\n", "y = data[1]\n", "\n", "# Create train test split\n", "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=2021)\n", "\n", "# Create a model\n", "model = LogisticRegression(solver='lbfgs')"]}, {"cell_type": "markdown", "metadata": {"index": 58}, "source": ["The metrics [found here](https://scikit-learn.org/stable/modules/model_evaluation.html) are built into sklearn's cross validation"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T21:18:33.507372Z", "start_time": "2021-04-08T21:18:33.401062Z"}, "index": 59}, "outputs": [], "source": ["from sklearn.model_selection import cross_val_score\n", "\n", "\n", "recall = cross_val_score(model, X_train, y_train, scoring='recall', cv=5)\n", "precision = cross_val_score(model, X_train, y_train, scoring='precision', cv=5)\n", "f1 = cross_val_score(model, X_train, y_train, scoring='f1', cv=5)\n", "accuracy = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-04-08T21:18:34.278590Z", "start_time": "2021-04-08T21:18:34.273518Z"}, "index": 60}, "outputs": [], "source": ["print('Average recall:', recall.mean())\n", "print('Average precision:', precision.mean())\n", "print('Average f1:', f1.mean())\n", "print('Average accuracy:', accuracy.mean())"]}, {"cell_type": "markdown", "metadata": {"index": 61}, "source": ["### Exercise\n", "\n", "If you wanted to use resampling with multiple fold cross validation, ***you cannot resample before making your cross validated splits***. When using resampling, you should always evaluate your model on *unbalanced* data. \n", "\n", "ie, the following would be *incorrect*\n", "```\n", "X_train, X_test, y_train, y_test = train_test_split(X,y)\n", "X_train, y_train = resample.fit_resample(X_train, y_train)\n", "\n", "scores = cross_val_score(model, X_train, y_train)\n", "```\n", "\n", "Because of this, you would need to write out the cross validation yourself using `Kfolds`.\n", "\n", "**In the cell below, comments have been provided to quide you through every step of this cross validation process.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 62}, "outputs": [], "source": ["# Import KFold from sklearn\n", "\n", "# Import precision, recall, \n", "# accuracy, and precision from sklearn\n", "\n", "\n", "# Create a kfolds object\n", "\n", "# Create a list for each \n", "# classification score\n", "\n", "\n", "# Instantiate a logistic regression model\n", "\n", "# Instantiate a standard scaler\n", "\n", "# Instantiate a smote object\n", "\n", "\n", "# Loop over the training and validation indices\n", "# using the kfolds object\n", "\n", "    # Split X_train into a train and validation split\n", "\n", "    \n", "    # Scale and transform the training \n", "    # split using the standard scaler\n", "\n", "    \n", "    # Resample the training split\n", "    \n", "    # Fit the model to the resampled training split\n", "    \n", "    # Scale the validation split\n", "\n", "    # Produce predictions for the validation split\n", "\n", "    # Calculate the recall, precision, f1, and accuracy scores\n", "\n", "    # Append the calculated scores to their corresponding list\n", "\n", "    \n", "# Print the mean of each score list    "]}, {"cell_type": "markdown", "metadata": {"index": 63}, "source": ["# Bonus!  ROC AUC and PR AUC\n", "\n", "**Receiver Operating Characterist Area Under the Curve** (ROC AUC)\n", "\n", "Probability threshold is the probability at which a classification model assigns an observation to a class.  By default this threshold is .5, but it can be changed.  Changing this threshold will change the metrics of the predictions.\n", "\n", "A ROC curve is a comparision, along different probability thresholds, of false positive rates and true positive rates.  We can use this metric to assess our model over all possible probability thresholds.  This gives a more complete way to compare different models when we are planning to tune the probability threshold we use to assign observations to classes in our predictions."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 64}, "outputs": [], "source": ["from sklearn.metrics import roc_auc_score, plot_roc_curve\n", "\n", "model.fit(X_train, y_train)\n", "\n", "#return only one of the columns returned by predict_proba, as they each add up to 1\n", "y_pred_proba = model.predict_proba(X_test)[:,1]\n", "roc_auc_score(y_test, y_pred_proba)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 65}, "outputs": [], "source": ["plot_roc_curve(model, X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {"index": 66}, "source": ["**Precision Recall Area Under the Curve**\n", "\n", "Similarly, this metric allows us to example the trade offs between precision and recall as we adjust the probability thresholds in our model."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 67}, "outputs": [], "source": ["from sklearn.metrics import plot_precision_recall_curve, average_precision_score\n", "\n", "average_precision_score(y_test, y_pred_proba)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 68}, "outputs": [], "source": ["plot_precision_recall_curve(model, X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {"index": 69}, "source": ["## The above tools are very helpful when working with imbalanced datasets\n", "\n", "In the real world most classification problems are imbalanced.  There are generally a large amount of the negative class and more of the positive class.  Consider customer churn, loan defaults, diseases, etc.  These kinds of datasets will have very skewed and misleading accuracies, recalls, and precisions.  F1 Score is difficult to interpret and better for model comparison.  They also give you information on the best way to set your probability thresholds."]}], "metadata": {"kernelspec": {"display_name": "learn-env", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}