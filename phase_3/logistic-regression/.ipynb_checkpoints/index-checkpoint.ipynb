{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    "# 25: Logistic Regression\n",
    "\n",
    "\n",
    "\n",
    "![](https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/styles/simple_image/public/images/machine-learning1.png?SnePeroHk5B9yZaLY7peFkULrfW8Gtaf&itok=yjEJbEKD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 1
   },
   "source": [
    "## Questions:\n",
    "\n",
    "**1. What is the difference between inferential modeling and predictive modeling?**\n",
    "\n",
    "**2. What is the difference between supervised learning and unsupervised learning?**\n",
    "\n",
    "**3. What is the difference between classification and regression?**\n",
    "\n",
    "Please type your answers below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": "Placeholder"
   },
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 4
   },
   "source": [
    "## What is Logistic Regression? \n",
    "\n",
    "![](https://miro.medium.com/max/400/1*zLfpo6F_Bfi6uvRL6iLX_Q.jpeg)\n",
    "It belongs to a class of predictive models called _Generalized Linear Models_. All of these models have 2 things in common: They all define significant relationships between independent/dependent variables and they indicate the strength of the relationships. \n",
    "\n",
    "Different from Linear regression -- it can predict the probabilities associated with **a success or a failure**. Is this email likely spam? What is the probability that this citizen will vote Republican? Is this homeowner likely to default on their mortgage? Is this person likely to buy our product? Is this tumor likely to be cancerous or benign?\n",
    "\n",
    "### Assumptions \n",
    "**Logistic Regression Assumptions:**\n",
    "\n",
    "* Binary logistic regression requires the dependent variable to be binary.\n",
    "* Only the meaningful variables should be included.\n",
    "* The independent variables should be independent of each other. That is, the model should have little or no multi-collinearity.\n",
    "* The independent variables are linearly related to the log odds.  For more about log odds vs probability, check out [this link.](https://www.statisticshowto.com/log-odds/)\n",
    "* Logistic regression requires quite large sample sizes.\n",
    "\n",
    "### Key differences from Linear Regression:\n",
    "* GLM does not assume a linear relationship between dependent and independent variables. However, it assumes a linear relationship between link function and independent variables in logit model.\n",
    "\n",
    "* The dependent variable need not to be normally distributed.\n",
    "\n",
    "* It does not uses OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE).\n",
    "\n",
    "* Errors need to be independent but not normally distributed.\n",
    "\n",
    "### Logistic Regression Equation\n",
    "\n",
    "![](https://miro.medium.com/max/571/0*tGVPGu3aa1rhTdfl.png)\n",
    "Let's say we've constructed our best-fit line, i.e. our linear predictor, $\\hat{L} = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$.\n",
    "\n",
    "#### The Sigmoid Function\n",
    "\n",
    "Consider the following transformation:\n",
    "$\\large\\hat{y} = \\Large\\frac{1}{1 + e^{-\\hat{L}}} \\large= \\Large\\frac{1}{1 + e^{-(\\beta_0 + ... + \\beta_nx_n)}}$. This is called the sigmoid function.\n",
    "\n",
    "This function squeezes our predictions between 0 and 1. \n",
    "\n",
    "Suppose I'm building a model to predict whether a plant is poisonous or not, based perhaps on certain biological features of its leaves. \n",
    "* I'll let '1' indicate a poisonous plant and '0' indicate a non-poisonous plant.\n",
    "* Now I'm forcing my predictions to be between 0 and 1, so suppose for test plant $P$ I get some value like 0.19.\n",
    "* I can naturally understand this as the probability that $P$ is poisonous.\n",
    "* If I truly want a binary prediction, I can simply round my score appropriately.\n",
    "\n",
    "How do we fit a line to our dependent variable if its values are already stored as probabilities? We can use the inverse of the sigmoid function, and just set our regression equation equal to that. The inverse of the sigmoid function is called the logit function, and it looks like this:\n",
    "\n",
    "$$\\large f(y) = \\ln\\left(\\frac{y}{1 - y}\\right)$$\n",
    "\n",
    "Notice that the domain of this function is $(0, 1)$.\n",
    "\n",
    "Quick proof that logit and sigmoid are inverse functions:\n",
    "\n",
    "$\\hspace{170mm}x = \\frac{1}{1 + e^{-y}}$;\n",
    "$\\hspace{170mm}$so $1 + e^{-y} = \\frac{1}{x}$;\n",
    "$\\hspace{170mm}$so $e^{-y} = \\frac{1 - x}{x}$;\n",
    "$\\hspace{170mm}$so $-y = \\ln\\left(\\frac{1 - x}{x}\\right)$;\n",
    "$\\hspace{170mm}$so $y = \\ln\\left(\\frac{x}{1 - x}\\right)$.)\n",
    "\n",
    "Our regression equation will now look like this:\n",
    "\n",
    "$\\large\\ln\\left(\\frac{y}{1 - y}\\right) = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 5
   },
   "source": [
    "## Coding Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 6
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import some data to play with\n",
    "from sklearn import datasets\n",
    "\n",
    "# For our modeling steps\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 7
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data= np.c_[iris['data'], iris['target']],\n",
    "    columns= iris['feature_names'] + ['target']\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 8
   },
   "outputs": [],
   "source": [
    "# Creating a large figure\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Iterating over the different features\n",
    "for i in range(0, 4):\n",
    "    # Figure number starts at 1\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    # Add a title to make it clear what each subplot shows\n",
    "    plt.title(df.columns[i])\n",
    "    # Use alpha to better see crossing pints\n",
    "    ax.scatter(df['target'], df.iloc[:,i], c='teal', alpha=0.1)\n",
    "    # Only show the tick marks for each target\n",
    "    plt.xticks(df.target.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 9
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 10
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12, solver='lbfgs', multi_class='auto')\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 11
   },
   "source": [
    "**What do you think 'multi_class=' means in the LogisticRegression() arguments?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 12
   },
   "outputs": [],
   "source": [
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 13
   },
   "outputs": [],
   "source": [
    "y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 14
   },
   "outputs": [],
   "source": [
    "residuals = y_train == y_hat_train\n",
    "\n",
    "print('Number of values correctly predicted:')\n",
    "print(pd.Series(residuals).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 15
   },
   "outputs": [],
   "source": [
    "residuals = y_test == y_hat_test\n",
    "\n",
    "print('Number of values correctly predicted: ')\n",
    "print(pd.Series(residuals).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, y_hat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
