{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use Ensemble Methods \n",
    "\n",
    "1. Traditional Decision Trees are prone to overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris['data'], columns = iris['feature_names'])\n",
    "data['target'] = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis = 1),\n",
    "                                                    data['target'], \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# fit on training data \n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# accuracy on training data \n",
    "print(f'Accuracy with training data: {dt.score(X_train, y_train)}')\n",
    "\n",
    "# accuracy on testing data \n",
    "print(f'Accuracy with testing data: {dt.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Provide high accuracy and are very popular \n",
    "> These visuals are from this [notebook](https://www.kaggle.com/antgoldbloom/what-algorithms-are-most-successful-on-kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Algorithms Used on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kaggle](Img/Most_Popular_Kaggle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Algorithms Used on Kaggle over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kaggle](Img/popular_kaggle_time_series.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy vs Interpretability \n",
    "![accuracy vs interpretability](Img/accuracy_interpretability.png)\n",
    "[Source](https://ff06-2020.fastforwardlabs.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "\n",
    "This strategy creates many 'weak' learners and combines them in some way to form a 'stronger' learner.  This draws on the 'wisdom of the crowds'.\n",
    "\n",
    "## Ensemble Method #1 - Bagging \n",
    "Bootstrap Aggregating.  Bootstrapping in this case means to take a random sample of features and/or observations with replacement for each estimator.  No one learner or estimator gets all of the data.  This prevents overfitting on each estimator, but the algorithm as a whole still trains on the entire dataset.\n",
    "\n",
    "![random_forest](Img/random-forest.png)\n",
    "[Source](https://www.javatpoint.com/machine-learning-random-forest-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps for Random Forest \n",
    "1. Select number of decision trees to build.  Sklearn by default does [100](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "2. Grab a random subset of the data points \n",
    "3. Train a decision tree on those data points \n",
    "4. Repeat Steps 2 and 3 for the number of trees you decided in step 1\n",
    "5. When predicting, each decision tree makes a separate prediction and the most common prediction is returned by the forest.\n",
    "> Important Note: the random forest only uses a subset of the features when making splits - this is the `max_features` hyperparameter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree 1 \n",
    "# sample of data \n",
    "train_data = X_train.join(y_train)\n",
    "data_1 = train_data.sample(frac = .3, replace = True, random_state=123)\n",
    "dt1 = DecisionTreeClassifier()\n",
    "dt1.fit(data_1.drop('target', axis = 1), data_1['target'])\n",
    "dt1_pred = dt1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree 2 \n",
    "# sample of data \n",
    "data_2 = train_data.sample(frac = .3, replace = True, random_state=456)\n",
    "dt2 = DecisionTreeClassifier()\n",
    "dt2.fit(data_2.drop('target', axis = 1), data_2['target'])\n",
    "dt2_pred = dt2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree 3 \n",
    "# sample of data \n",
    "data_3 = train_data.sample(frac = .3, replace = True, random_state=789)\n",
    "dt3 = DecisionTreeClassifier()\n",
    "dt3.fit(data_3.drop('target', axis = 1), data_3['target'])\n",
    "dt3_pred = dt3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({'dt1': dt1_pred, 'dt2': dt2_pred, 'dt3': dt3_pred})\n",
    "df_pred['true'] = np.array(y_test)\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point 29 has different predictions, so by using a random forest we got it correct \n",
    "df_pred.iloc[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest in sklearn \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# accuracy on training data \n",
    "print(f'Accuracy with training data: {rf.score(X_train, y_train)}')\n",
    "\n",
    "# accuracy on testing data \n",
    "print(f'Accuracy with testing data: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (8,8), dpi=300)\n",
    "tree.plot_tree(rf.estimators_[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Method #2 - Boosting\n",
    "1. Train a single learner\n",
    "2. Figure out which examples the learner got wrong\n",
    "3. Build another strong learner that focuses on the areas the first learner got wrong\n",
    "4. Continue this process until a predetermined stopping condition is met, such as until a set number of learners have been created, or the model's performance has plateaued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost\n",
    "<img src='Img/new_adaboost.png' width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Each data point has a weight assigned at the beginning all weights are equal \n",
    "2. A subset of the data is grabbed based on the weights \n",
    "3. A weak model is trained on the data \n",
    "    - If model gets the data point correct the weight is decreased\n",
    "    - If model gets the data point incorrect the weight is increased \n",
    "4. Points that the model got incorrect then have a higher probability of being grabbed for the next model.\n",
    "> This causes the harder points (the ones it gets incorrect) to be chosen more often "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# accuracy on training data \n",
    "print(f'Accuracy with training data: {ada.score(X_train, y_train)}')\n",
    "\n",
    "# accuracy on testing data \n",
    "print(f'Accuracy with testing data: {ada.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ada.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (8,8), dpi=100)\n",
    "tree.plot_tree(ada.estimators_[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting \n",
    "<img src='Img/new_gradient-boosting.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit a model (model 1) to the data \n",
    "2. Fit a model (model 2) to the residuals from model 1\n",
    "3. Combine the 2 models \n",
    "4. Repeat steps 2 and 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Code adapted from https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit regression line to this data \n",
    "\n",
    "Start with fitting 3 decision trees to the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to the residuals of first decision tree\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to the residuals of the second decision tree \n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0.8]])\n",
    "\n",
    "y1_pred = tree_reg1.predict(X_new)\n",
    "y2_pred = tree_reg2.predict(X_new)\n",
    "y3_pred = tree_reg3.predict(X_new)\n",
    "\n",
    "print(f'Prediction from first decision tree {y1_pred[0]}')\n",
    "print(f'Prediction from second decision tree {y2_pred[0]}')\n",
    "print(f'Prediction from third decision tree {y3_pred[0]}')\n",
    "print('')\n",
    "print(f'Overall Prediction: {y1_pred[0] + y2_pred[0] + y3_pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(regressors, X, y):\n",
    "    axes=[-0.5, 0.5, -0.1, 0.8]\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.scatter(X[:, 0], y)\n",
    "    plt.plot(x1, y_pred, linewidth=2, color = 'k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first tree \n",
    "plot_predictions([tree_reg1], X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second tree \n",
    "plot_predictions([tree_reg2], X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining tree 1 and tree 2\n",
    "plot_predictions([tree_reg1, tree_reg2], X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third tree \n",
    "plot_predictions([tree_reg3], X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining trees 1, 2, and 3 \n",
    "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting vs Underfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, random_state=42)\n",
    "gbrt.fit(X, y)\n",
    "\n",
    "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, random_state=42)\n",
    "gbrt_slow.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions([gbrt], X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions([gbrt_slow], X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# accuracy on training data \n",
    "print(f'Accuracy with training data: {gb.score(X_train, y_train)}')\n",
    "\n",
    "# accuracy on testing data \n",
    "print(f'Accuracy with testing data: {gb.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "- [StatQuest video on Random Forests](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)\n",
    "- [StatQuest video on AdaBoost](https://www.youtube.com/watch?v=LsK-xG1cLYA)\n",
    "- [StatQuest video on Gradient Boosting](https://www.youtube.com/watch?v=3CC4N4z3GJc)\n",
    "- [Great Book on ML in sklearn](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_4?dchild=1&keywords=Hands-On+Machine+Learning+with+Scikit-Learn+and+TensorFlow%3A+Concepts%2C+Tools%2C+and+Techniques+to+Build+Intelligent+Systems&qid=1618509260&s=books&sr=1-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
